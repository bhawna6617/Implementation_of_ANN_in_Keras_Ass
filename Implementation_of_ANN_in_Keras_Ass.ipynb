{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f1bb17",
   "metadata": {},
   "source": [
    "# Install and load the latest versions of TensorFlow and Keras. Print their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c99f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install TensorFlow and Keras\n",
    "# !pip install tensorflow keras\n",
    "\n",
    "# # Import TensorFlow and Keras\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "# # Print TensorFlow and Keras versions\n",
    "# print(\"TensorFlow version:\", tf.version.VERSION)\n",
    "# print(\"Keras version:\", keras.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fc112d",
   "metadata": {},
   "source": [
    "# Check for null values, identify categorical variables, and encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c365ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Load your dataset into a Pandas DataFrame\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# # Check for null values\n",
    "# print(\"Null values in the dataset:\")\n",
    "# print(df.isnull().sum())\n",
    "\n",
    "# # Identify categorical variables\n",
    "# categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "# print(\"Categorical columns:\")\n",
    "# print(categorical_cols)\n",
    "\n",
    "# # Encode categorical variables using LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# for col in categorical_cols:\n",
    "#     df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# print(\"Encoded dataset:\")\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4bd29f",
   "metadata": {},
   "source": [
    "# Separate the features and target variables from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e07178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load your dataset into a Pandas DataFrame\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# # Identify the target variable (assuming it's the last column)\n",
    "# target_var = df.columns[-1]\n",
    "\n",
    "# # Separate the features and target variables\n",
    "# X = df.drop(columns=[target_var])  # features\n",
    "# y = df[target_var]  # target variable\n",
    "\n",
    "# print(\"Features (X):\")\n",
    "# print(X.head())\n",
    "\n",
    "# print(\"Target variable (y):\")\n",
    "# print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ac61e1",
   "metadata": {},
   "source": [
    "# Perform a train-test split, dividing the data into training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61251ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load your dataset into a Pandas DataFrame\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# # Identify the target variable (assuming it's the last column)\n",
    "# target_var = df.columns[-1]\n",
    "\n",
    "# # Separate the features and target variables\n",
    "# X = df.drop(columns=[target_var])  # features\n",
    "# y = df[target_var]  # target variable\n",
    "\n",
    "# # Perform a train-test split, dividing the data into training, validation, and test datasets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)\n",
    "\n",
    "# print(\"Training dataset (X_train, y_train):\")\n",
    "# print(X_train.shape, y_train.shape)\n",
    "\n",
    "# print(\"Validation dataset (X_val, y_val):\")\n",
    "# print(X_val.shape, y_val.shape)\n",
    "\n",
    "# print(\"Test dataset (X_test, y_test):\")\n",
    "# print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e128e",
   "metadata": {},
   "source": [
    "# Scale the dataset using an appropriate scaling technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "511a7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Load your dataset into a Pandas DataFrame\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# # Identify the target variable (assuming it's the last column)\n",
    "# target_var = df.columns[-1]\n",
    "\n",
    "# # Separate the features and target variables\n",
    "# X = df.drop(columns=[target_var])  # features\n",
    "# y = df[target_var]  # target variable\n",
    "\n",
    "# # Perform a train-test split, dividing the data into training, validation, and test datasets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)\n",
    "\n",
    "# # Scale the features using StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_val_scaled = scaler.transform(X_val)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# print(\"Scaled training dataset (X_train_scaled):\")\n",
    "# print(X_train_scaled.shape)\n",
    "\n",
    "# print(\"Scaled validation dataset (X_val_scaled):\")\n",
    "# print(X_val_scaled.shape)\n",
    "\n",
    "# print(\"Scaled test dataset (X_test_scaled):\")\n",
    "# print(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d81b2b",
   "metadata": {},
   "source": [
    "# Design and implement at least two hidden layers and an output layer for the binary categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "733cc592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# # Load your dataset into a Pandas DataFrame\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# # Identify the target variable (assuming it's the last column)\n",
    "# target_var = df.columns[-1]\n",
    "\n",
    "# # Separate the features and target variables\n",
    "# X = df.drop(columns=[target_var])  # features\n",
    "# y = df[target_var]  # target variable\n",
    "\n",
    "# # Perform a train-test split, dividing the data into training, validation, and test datasets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)\n",
    "\n",
    "# # Scale the features using StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_val_scaled = scaler.transform(X_val)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Define the neural network model\n",
    "# model = Sequential()\n",
    "\n",
    "# # Hidden layer 1: 64 units, ReLU activation\n",
    "# model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "\n",
    "# # Hidden layer 2: 32 units, ReLU activation\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# # Output layer: 1 unit, sigmoid activation (for binary classification)\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_val_scaled, y_val))\n",
    "\n",
    "# # Evaluate the model on the test dataset\n",
    "# loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "# print(f'Test accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ffd24",
   "metadata": {},
   "source": [
    "# Create a Sequential model in Keras and add the previously designed layers to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e595474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# # Create a Sequential model\n",
    "# model = Sequential()\n",
    "\n",
    "# # Add the first hidden layer with 64 units and ReLU activation\n",
    "# model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "\n",
    "# # Add the second hidden layer with 32 units and ReLU activation\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# # Add the output layer with 1 unit and sigmoid activation\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32838fa4",
   "metadata": {},
   "source": [
    "# Print the summary of the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44e11dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: \"sequential\"\n",
    "# _________________________________________________________________\n",
    "# Layer (type)                 Output Shape              Param #   \n",
    "# =================================================================\n",
    "# dense (Dense)                (None, 64)               320       \n",
    "# _________________________________________________________________\n",
    "# dense_1 (Dense)             (None, 32)               2080      \n",
    "# _________________________________________________________________\n",
    "# dense_2 (Dense)             (None, 1)                33        \n",
    "# =================================================================\n",
    "# Total params: 2433\n",
    "# Trainable params: 2433\n",
    "# Non-trainable params: 0\n",
    "# _________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb6aa2",
   "metadata": {},
   "source": [
    "# Set the loss function(‘binary_crossentropy’), optimizer, and include the accuracy metric in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c8d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='binary_crossentropy', \n",
    "#               optimizer='adam', \n",
    "#               metrics=['accuracy'])\n",
    "# loss='binary_crossentropy': This sets the loss function to binary cross-entropy, which is suitable for binary classification problems where the target variable is a binary label (0 or 1).\n",
    "# optimizer='adam': This sets the optimizer to Adam, which is a popular stochastic gradient descent algorithm that adapts the learning rate for each parameter based on the magnitude of the gradient.\n",
    "# metrics=['accuracy']: This includes the accuracy metric in the model, which measures the proportion of correctly classified samples.\n",
    "#     from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# # Create a Sequential model\n",
    "# model = Sequential()\n",
    "\n",
    "# # Add the first hidden layer with 64 units and ReLU activation\n",
    "# model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "\n",
    "# # Add the second hidden layer with 32 units and ReLU activation\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# # Add the output layer with 1 unit and sigmoid activation\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='binary_crossentropy', \n",
    "#               optimizer='adam', \n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ae5f03",
   "metadata": {},
   "source": [
    "\n",
    "# Compile the model with the specified loss function, optimizer, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d23cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='binary_crossentropy', \n",
    "#               optimizer='adam', \n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040f4df3",
   "metadata": {},
   "source": [
    "# Fit the model to the training data using appropriate batch size and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3aff597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# # Create a Sequential model\n",
    "# model = Sequential()\n",
    "\n",
    "# # Add the first hidden layer with 64 units and ReLU activation\n",
    "# model.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "\n",
    "# # Add the second hidden layer with 32 units and ReLU activation\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# # Add the output layer with 1 unit and sigmoid activation\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='binary_crossentropy', \n",
    "#               optimizer='adam', \n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Fit the model to the training data\n",
    "# history = model.fit(X_train_scaled, y_train, \n",
    "#                     batch_size=32, \n",
    "#                     epochs=10, \n",
    "#                     validation_data=(X_val_scaled, y_val))\n",
    "\n",
    "# print(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34efb97e",
   "metadata": {},
   "source": [
    "# Obtain the model's parameters (weights and biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0672d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the model's parameters (weights and biases)\n",
    "# model_parameters = model.get_weights()\n",
    "\n",
    "# # Print the model's parameters\n",
    "# for i, parameter in enumerate(model_parameters):\n",
    "#     print(f\"Parameter {i}: {parameter.shape}\")\n",
    "#     print(parameter)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c452f4",
   "metadata": {},
   "source": [
    "# Store the model's training history as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd3c750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Get the model's training history\n",
    "# history = model.fit(X_train_scaled, y_train, \n",
    "#                     batch_size=32, \n",
    "#                     epochs=10, \n",
    "#                     validation_data=(X_val_scaled, y_val))\n",
    "\n",
    "# # Convert the history to a Pandas DataFrame\n",
    "# history_df = pd.DataFrame(history.history)\n",
    "\n",
    "# # Print the DataFrame\n",
    "# print(history_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f03a415",
   "metadata": {},
   "source": [
    "# Evaluate the model's performance using the test dataset and report relevant metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53d644f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model on the test dataset\n",
    "# test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "# # Print the test loss and accuracy\n",
    "# print(f\"Test loss: {test_loss:.3f}\")\n",
    "# print(f\"Test accuracy: {test_accuracy:.3f}\")\n",
    "\n",
    "# # Get the predicted probabilities\n",
    "# y_pred_prob = model.predict(X_test_scaled)\n",
    "\n",
    "# # Get the predicted classes\n",
    "# y_pred_class = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# # Calculate the confusion matrix\n",
    "# conf_mat = confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "# # Print the confusion matrix\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_mat)\n",
    "\n",
    "# # Calculate the classification report\n",
    "# class_report = classification_report(y_test, y_pred_class)\n",
    "\n",
    "# # Print the classification report\n",
    "# print(\"Classification Report:\")\n",
    "# print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e8081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
